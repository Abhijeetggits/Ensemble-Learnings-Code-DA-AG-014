{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "XHCixPh9-gIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "    behind it.\n",
        "\n",
        "    -> Ensemble Learning is a machine learning technique that combines   predictions from multiple models to improve overall performance.\n",
        "    Key Idea: Combine weak learners to form a strong learner. Common approaches:\n",
        "        Bagging (Bootstrap Aggregation) – reduces variance\n",
        "\n",
        "Bo      osting – reduces bias"
      ],
      "metadata": {
        "id": "yvwkGFYY_Pdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "  -> Bagging: Builds multiple models in parallel using bootstrap samples to reduce variance.\n",
        "\n",
        "   Boosting: Builds models sequentially, each correcting previous errors, to reduce bias and variance."
      ],
      "metadata": {
        "id": "y912cWmq_lPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. : What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "  -> Bootstrap sampling is a technique where multiple datasets are created from the original dataset by randomly sampling with replacement. This means some data points may appear multiple times, while others may be left out in each sample.\n",
        "  \n",
        "  Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "   Used to train each individual model (like each decision tree) on a different subset of data.\n",
        "\n",
        "   Introduces diversity among the models, which helps reduce variance and improves overall model stability and accuracy.\n",
        "\n",
        "   The samples not selected in a bootstrap sample are called Out-of-Bag (OOB) samples and can be used to estimate model performance without a separate validation set.\n"
      ],
      "metadata": {
        "id": "BPXE_3hxAE2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. : What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "  -> Out-of-Bag (OOB) Samples:\n",
        "OOB samples are the data points not included in a bootstrap sample for a particular model in Bagging (like a tree in Random Forest).\n",
        "  \n",
        "  OOB Score:\n",
        "\n",
        "   After training, the model predicts labels for its OOB samples.\n",
        "\n",
        "   The OOB score is the average accuracy (or error) on all OOB samples across all models.\n",
        "\n",
        "   It provides an internal validation metric without needing a separate test set, helping evaluate the ensemble model’s performance."
      ],
      "metadata": {
        "id": "pQQxRiqUAVcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "  -> Single Decision Tree:\n",
        "     * Feature importance is calculated based on how much a feature reduces impurity (like Gini or entropy) at each split.\n",
        "     * can be unstable, as small changes in data may drastically change the tree structure and importance values.\n",
        "\n",
        "\n",
        "\n",
        "  Random Forest:\n",
        "    * Feature importance is averaged over all trees in the forest, making it more stable and reliable.\n",
        "\n",
        "    * Captures feature contributions better because it considers multiple bootstrap samples and random feature subsets."
      ],
      "metadata": {
        "id": "kXAn_EAcAkY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.  Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "#  sklearn.datasets.load_breast_cancer()\n",
        "#  ● Train a Random Forest Classifier\n",
        "#  ● Print the top 5 most important features based on feature importance scores.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "top5_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwPA_oSCB2_7",
        "outputId": "00c032f9-17e0-4fb1-838c-ddda19d935a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy of single Decision Tree: {accuracy_dt:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bag:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuiVmVeWCEx6",
        "outputId": "68ac2c03-b32c-4372-daaa-9f097e0f44c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.n 8: Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 2, 4, 6]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate final model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0AXo6_6CjtI",
        "outputId": "0fbdcc3d-e068-429e-8cee-74d61c1aeb9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "# Train Random Forest Regressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {mse_bag:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rppcExhNCy8U",
        "outputId": "c373556a-ea59-43db-ae65-d58c2454c7ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2568\n",
            "Mean Squared Error of Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "contex\n",
        "\n",
        "  -> 1.Choose between Bagging or Boosting:\n",
        "\n",
        "    * Bagging (e.g., Random Forest) is suitable if the dataset is large and prone to high variance, helping stabilize predictions.\n",
        "\n",
        "    * Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM) is ideal if the dataset shows bias errors and we want to sequentially improve weak learners.\n",
        "\n",
        "    * I would start with Boosting, as predicting loan default often benefits from reducing bias and focusing on misclassified “high-risk” cases.\n",
        "\n",
        "  2. Handle Overfitting:\n",
        "     * Use cross-validation to monitor performance.\n",
        "     * Limit model complexity with max_depth, min_samples_split, and regularization parameters.\n",
        "     * Apply early stopping in boosting methods.\n",
        "\n",
        "  3. Select Base Models:\n",
        "\n",
        "     * For Bagging, typically Decision Trees are used due to high variance and their suitability for structured/tabular data.\n",
        "\n",
        "     * For Boosting, Decision Trees with shallow depth are preferred to avoid overfitting while allowing sequential learning.\n",
        "\n",
        "  4. Evaluate Performance Using Cross-Validation:\n",
        "     Use k-fold cross-validation (e.g., 5 or 10 folds) to estimate the model’s performance robustly.\n",
        "    Track metrics relevant to loan default, such as accuracy, precision, recall, F1-score, and AUC-ROC, especially since the dataset may be imbalanced.\n",
        "\n",
        "\n",
        "  5. Justify How Ensemble Learning Improves Decision-Making:\n",
        "\n",
        "    * Ensemble methods combine multiple models to reduce variance, bias, or both, resulting in more accurate and stable predictions.\n",
        "\n",
        "    * In financial institutions, this means better identification of high-risk borrowers, reducing default rates and financial loss.\n",
        "\n",
        "    * Ensembles help mitigate the risk of relying on a single model that might underperform on certain customer segments, making decision-making more robust and reliable."
      ],
      "metadata": {
        "id": "HwGHJzOzDhla"
      }
    }
  ]
}